{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : In the agent.update() function, only make one time to sense the environment to get the state . And then transfer the state as the previous state in the end of the function .\n",
    "\n",
    "### And the score is below ,the highest one is 9% success rate when epsilon is 3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1:   changes in agent.update ()  \n",
    "```\n",
    "    def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "        \n",
    "        # TODO: Current state\n",
    "        self.state_next = inputs\n",
    "        self.state_next['next_waypoint'] = self.next_waypoint\n",
    "        self.state_next = tuple(sorted(self.state_next.items()))\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        action = self.car_AI.bestAction(self.state)\n",
    "        \n",
    "        # Execute action and get reward\n",
    "        #if agent reached, self.done = True ,and then break in env.act().\n",
    "        reward = self.env.act(self, action)  \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # TODO: update previous Q\n",
    "        self.car_AI.iterLearn(self.state, self.action, reward, self.state_next)\n",
    "\n",
    "        #update state\n",
    "        self.total_paces += 1\n",
    "        self.action = action\n",
    "        self.state = self.state_next\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_good = pd.read_csv('alpha_gamma_gridsearch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>alpha</th>\n",
       "      <th>avg_total_deadline</th>\n",
       "      <th>avg_total_paces</th>\n",
       "      <th>avg_total_penalties</th>\n",
       "      <th>avg_total_reward</th>\n",
       "      <th>gamma</th>\n",
       "      <th>total_reached_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.35</td>\n",
       "      <td>29.285714</td>\n",
       "      <td>14.560440</td>\n",
       "      <td>1.406593</td>\n",
       "      <td>28.758242</td>\n",
       "      <td>0.35</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.20</td>\n",
       "      <td>30.533333</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>30.213333</td>\n",
       "      <td>0.20</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.20</td>\n",
       "      <td>33.018868</td>\n",
       "      <td>17.622642</td>\n",
       "      <td>2.075472</td>\n",
       "      <td>29.075472</td>\n",
       "      <td>0.50</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.65</td>\n",
       "      <td>30.384615</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>3.673077</td>\n",
       "      <td>23.586538</td>\n",
       "      <td>0.65</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.50</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>15.040000</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>25.660000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  alpha  avg_total_deadline  avg_total_paces  \\\n",
       "16          16   0.35           29.285714        14.560440   \n",
       "8            8   0.20           30.533333        17.066667   \n",
       "10          10   0.20           33.018868        17.622642   \n",
       "32          32   0.65           30.384615        15.500000   \n",
       "23          23   0.50           31.400000        15.040000   \n",
       "\n",
       "    avg_total_penalties  avg_total_reward  gamma  total_reached_times  \n",
       "16             1.406593         28.758242   0.35                   91  \n",
       "8              0.666667         30.213333   0.20                   75  \n",
       "10             2.075472         29.075472   0.50                   53  \n",
       "32             3.673077         23.586538   0.65                   52  \n",
       "23             2.060000         25.660000   0.35                   50  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_good.sort_values(['total_reached_times'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : In the agent.update() function, only make one time to sense the environment to get the state . And then transfer the state as the previous state in the end of the function .\n",
    "\n",
    "### And the score is below ,the highest one is 88% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_3params = pd.read_csv('824Great_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>alpha</th>\n",
       "      <th>avg_total_deadline</th>\n",
       "      <th>avg_total_paces</th>\n",
       "      <th>avg_total_penalties</th>\n",
       "      <th>avg_total_reward</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>gamma</th>\n",
       "      <th>total_reached_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.10</td>\n",
       "      <td>29.829545</td>\n",
       "      <td>14.227273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>29.585227</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.45</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.05</td>\n",
       "      <td>29.812500</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>29.350000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.35</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>30.064103</td>\n",
       "      <td>17.256410</td>\n",
       "      <td>1.179487</td>\n",
       "      <td>31.224359</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.70</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>367</td>\n",
       "      <td>0.35</td>\n",
       "      <td>29.144737</td>\n",
       "      <td>14.973684</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>29.611842</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.45</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>0.10</td>\n",
       "      <td>29.605263</td>\n",
       "      <td>16.197368</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>30.717105</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.55</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  alpha  avg_total_deadline  avg_total_paces  \\\n",
       "81           81   0.10           29.829545        14.227273   \n",
       "18           18   0.05           29.812500        14.850000   \n",
       "98           98   0.10           30.064103        17.256410   \n",
       "367         367   0.35           29.144737        14.973684   \n",
       "87           87   0.10           29.605263        16.197368   \n",
       "\n",
       "     avg_total_penalties  avg_total_reward  epsilon  gamma  \\\n",
       "81              0.227273         29.585227     0.03   0.45   \n",
       "18              0.962500         29.350000     0.03   0.35   \n",
       "98              1.179487         31.224359     0.10   0.70   \n",
       "367             0.881579         29.611842     0.05   0.45   \n",
       "87              0.842105         30.717105     0.03   0.55   \n",
       "\n",
       "     total_reached_times  \n",
       "81                    88  \n",
       "18                    80  \n",
       "98                    78  \n",
       "367                   76  \n",
       "87                    76  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3params.sort_values(['total_reached_times'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But\n",
    "### When I apply the method 2 : setting a current state  and  a next state in agent.update( ) to iterlearn Q-table.\n",
    "### But the reached_times is too low to compare with the above data.\n",
    "### Why???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2:   changes in agent.update ()  \n",
    "```\n",
    " def update(self, t):\n",
    "        # Gather inputs\n",
    "        self.next_waypoint = self.planner.next_waypoint()  # from route planner, also displayed by simulator\n",
    "        inputs = self.env.sense(self)\n",
    "        deadline = self.env.get_deadline(self)\n",
    "        \n",
    "        # TODO: Current state\n",
    "        self.state = inputs\n",
    "        self.state['next_waypoint'] = self.next_waypoint\n",
    "        self.state = tuple(sorted(self.state.items()))\n",
    "        \n",
    "        # TODO: Select action according to your policy\n",
    "        action = self.car_AI.bestAction(self.state)\n",
    "        \n",
    "        # Execute action and get reward\n",
    "        #if agent reached, self.done = True ,and then break in env.act().\n",
    "        reward = self.env.act(self, action)  \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # TODO: Get next state\n",
    "        inputs_next = self.env.sense(self)\n",
    "        self.state_next = inputs_next\n",
    "        self.state_next['next_waypoint'] = self.planner.next_waypoint() \n",
    "        self.state_next = tuple(sorted(self.state_next.items()))\n",
    "        \n",
    "        # TODO: update previous Q\n",
    "        self.car_AI.iterLearn(self.state, self.action, reward, self.state_next)\n",
    "\n",
    "        #update state\n",
    "        self.total_paces += 1\n",
    "        self.action = action\n",
    "        self.state = self.state_next\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_825 = pd.read_csv('BBB825Great_grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>alpha</th>\n",
       "      <th>avg_total_deadline</th>\n",
       "      <th>avg_total_paces</th>\n",
       "      <th>avg_total_penalties</th>\n",
       "      <th>avg_total_reward</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>gamma</th>\n",
       "      <th>total_reached_times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>266</td>\n",
       "      <td>0.95</td>\n",
       "      <td>31.451613</td>\n",
       "      <td>18.967742</td>\n",
       "      <td>3.967742</td>\n",
       "      <td>23.564516</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>33.035714</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>3.607143</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>0.65</td>\n",
       "      <td>35.769231</td>\n",
       "      <td>20.115385</td>\n",
       "      <td>5.038462</td>\n",
       "      <td>21.346154</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>0.60</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>20.160000</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>25.480000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>0.65</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>17.520000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  alpha  avg_total_deadline  avg_total_paces  \\\n",
       "266         266   0.95           31.451613        18.967742   \n",
       "59           59   0.60           33.035714        16.250000   \n",
       "101         101   0.65           35.769231        20.115385   \n",
       "74           74   0.60           29.800000        20.160000   \n",
       "92           92   0.65           31.400000        17.520000   \n",
       "\n",
       "     avg_total_penalties  avg_total_reward  epsilon  gamma  \\\n",
       "266             3.967742         23.564516      0.1   0.40   \n",
       "59              3.607143         20.125000      0.1   0.10   \n",
       "101             5.038462         21.346154      0.1   0.35   \n",
       "74              3.720000         25.480000      0.1   0.35   \n",
       "92              3.160000         22.700000      0.1   0.20   \n",
       "\n",
       "     total_reached_times  \n",
       "266                   31  \n",
       "59                    28  \n",
       "101                   26  \n",
       "74                    25  \n",
       "92                    25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_825.sort_values(['total_reached_times'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
